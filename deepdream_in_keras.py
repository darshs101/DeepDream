# -*- coding: utf-8 -*-
"""DeepDream in Keras

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rmvUK-tq4Zqgk5CWFiQPgxy-5bQMYILl

Fetching the test image
"""

from tensorflow import keras
import matplotlib.pyplot as plt

base_image_path = keras.utils.get_file(
    "coast.jpg", origin="https://img-datasets.s3.amazonaws.com/coast.jpg")

plt.axis("off")
plt.imshow(keras.utils.load_img(base_image_path))

"""Instantiating a pretrained InceptionV3 model"""

from tensorflow.keras.applications import inception_v3
model = inception_v3.InceptionV3(weights="imagenet", include_top=False)

"""Configuring the contribution of each layer to the DeepDream loss"""

#layers for which we try to maximize activation as well as their weight in the total loss.
#you can tweak these settings to obtain new visual effects
layer_settings = {
    "mixed4": 1.0,
    "mixed5": 1.5,
    "mixed6": 2.0,
    "mixed7": 2.5,
}
#symbolic outputs of each layer
outputs_dict = dict(
    [
        (layer.name, layer.output)
        for layer in [model.get_layer(name) for name in layer_settings.keys()]
    ]
)
#model that returns the activation values for every target layer (as a dict)
feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)

"""The DeepDream loss"""

def compute_loss(input_image):
    #extract activations
    features = feature_extractor(input_image)

    #initialize the loss to 0
    loss = tf.zeros(shape=())
    for name in features.keys():
        coeff = layer_settings[name]
        activation = features[name]

        #avoid border artifacts by only involving non-border pixels in the loss
        loss += coeff * tf.reduce_mean(tf.square(activation[:, 2:-2, 2:-2, :]))
    return loss

"""The DeepDream gradient ascent process"""

import tensorflow as tf

#make the trainign step fast by compiling it as a tf.function
@tf.function
def gradient_ascent_step(image, learning_rate):

    #compute gradients of DeepDream loss with respect to the current image
    with tf.GradientTape() as tape:
        tape.watch(image)
        loss = compute_loss(image)
    grads = tape.gradient(loss, image)

    #normalize gradients
    grads = tf.math.l2_normalize(grads)
    image += learning_rate * grads
    return loss, image


def gradient_ascent_loop(image, iterations, learning_rate, max_loss=None):
    #repeatedly update the image in a way that increases the DeepDream loss
    for i in range(iterations):
        loss, image = gradient_ascent_step(image, learning_rate)

        #break out if the loss crosses a certain threshold
        #(over-optimizing would create unwanted image artifacts)
        if max_loss is not None and loss > max_loss:
            break
        print(f"... Loss value at step {i}: {loss:.2f}")
    return image

"""Parameters for Gradient Ascent"""

#gradient ascent step size
step = 20.

#number of scales at which to run gradient ascent
num_octave = 3

#size ratio between successive scales
octave_scale = 1.4

#number of gradient ascent steps per scale
iterations = 30

#stop the gradient ascent process for a scale if the loss gets higher than this
max_loss = 15.

"""Image processing utilities"""

import numpy as np

#util function to open, resize, and format pictures into appropriate arrays
def preprocess_image(image_path):
    img = keras.utils.load_img(image_path)
    img = keras.utils.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = keras.applications.inception_v3.preprocess_input(img)
    return img

#util function to convert a numpy array into a valid image
def deprocess_image(img):
    img = img.reshape((img.shape[1], img.shape[2], 3))
    img /= 2.0
    img += 0.5
    img *= 255.

    #convert to unint8 and clip to the valid range [0,255]
    img = np.clip(img, 0, 255).astype("uint8")
    return img

"""Running gradient ascent over multiple successive "octaves"
"""

#load the test image
original_img = preprocess_image(base_image_path)
original_shape = original_img.shape[1:3]

#compute the target shape of the image at different octaves
successive_shapes = [original_shape]
for i in range(1, num_octave):
    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])
    successive_shapes.append(shape)
successive_shapes = successive_shapes[::-1]

shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])

#make a copy of the image (to keep the original around)
img = tf.identity(original_img)

#iterate over the different octaves
for i, shape in enumerate(successive_shapes):
    print(f"Processing octave {i} with shape {shape}")

    #scale up the dream image
    img = tf.image.resize(img, shape)

    #run gradient ascent, altering the dream
    img = gradient_ascent_loop(
        img, iterations=iterations, learning_rate=step, max_loss=max_loss
    )

    #scale up the smaller version of the original image; it will be pixellated
    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)

    #compute the high-quality version of the original image at this size
    same_size_original = tf.image.resize(original_img, shape)

    #the difference between the two is the detail that was lost when scaling up
    lost_detail = same_size_original - upscaled_shrunk_original_img

    #re-inject lost detail into the dream
    img += lost_detail
    shrunk_original_img = tf.image.resize(original_img, shape)

#save the final result
keras.utils.save_img("dream.png", deprocess_image(img.numpy()))